# ğŸ™ Voice Agent

A fully local, privacy-first voice AI assistant built with:

| Component | Technology |
|-----------|-----------|
| **LLM** | Ollama (llama3.2, mistral, etc.) |
| **STT** | Whisper (faster-whisper) |
| **TTS** | Edge-TTS (Microsoft Azure free tier) |
| **Embeddings** | Ollama nomic-embed-text |
| **Vector DB** | FAISS |
| **Memory** | Redis + in-memory conversation store |
| **Backend** | FastAPI + WebSockets |
| **Frontend** | Vanilla TypeScript |
| **Infra** | Docker Compose + Nginx |

---

## Architecture

```
Browser
  â”‚  WebSocket (PCM audio stream)
  â–¼
Nginx
  â”‚  Reverse proxy
  â–¼
FastAPI (voice-agent)
  â”‚
  â”œâ”€â”€ STT  â†’  faster-whisper  â†’  transcript text
  â”‚
  â”œâ”€â”€ RAG  â†’  FAISS + nomic-embed-text  â†’  relevant context
  â”‚
  â”œâ”€â”€ Memory  â†’  Redis conversation store  â†’  history context
  â”‚
  â”œâ”€â”€ LLM  â†’  Ollama (llama3.2)  â†’  response text (streaming)
  â”‚
  â””â”€â”€ TTS  â†’  edge-tts  â†’  audio stream back to browser
```

---

## Prerequisites

| Tool | Version | Install |
|------|---------|---------|
| Docker | 24+ | https://docs.docker.com/get-docker/ |
| Docker Compose | v2 | Included with Docker Desktop |
| Git | any | https://git-scm.com |

> **RAM**: 8GB minimum (16GB recommended for larger models)
> **Disk**: 10GB free (models are large)

---

## ğŸš€ Quick Start (Docker â€” Recommended)

### Step 1 â€” Clone the repository

```bash
git clone <your-repo-url>
cd voice-agent
```

### Step 2 â€” Create your environment file

```bash
cp .env.example .env
```

Edit `.env` if needed (defaults work out of the box).

### Step 3 â€” Start all services

```bash
cd infra/docker
docker compose up -d
```

This starts:
- **ollama** â€” LLM server (pulls `llama3.2` + `nomic-embed-text` automatically)
- **redis** â€” cache + session memory
- **voice-agent** â€” FastAPI backend
- **nginx** â€” reverse proxy on port 80

### Step 4 â€” Wait for models to download

The first run downloads AI models (~2-4GB). Monitor progress:

```bash
docker compose logs -f ollama-init
# Wait until you see: "=== All models ready ==="
```

Check that all services are healthy:

```bash
docker compose ps
# All should show "healthy" or "Up"
```

### Step 5 â€” Open the UI

Navigate to **http://localhost** in your browser.

> âš ï¸ **HTTPS Required for Microphone**: Chrome/Firefox block microphone access on plain HTTP except for `localhost`. If deploying to a server, set up HTTPS (see [HTTPS section](#https-setup)).

### Step 6 â€” Talk to your voice agent!

1. Click the **microphone button** ğŸ¤
2. Speak your question
3. Pause for ~1 second â€” the agent auto-detects silence
4. Listen to the response

---

## ğŸ–¥ï¸ Local Development (No Docker)

### Step 1 â€” Install system dependencies

**macOS:**
```bash
brew install ffmpeg redis
```

**Ubuntu/Debian:**
```bash
sudo apt install ffmpeg redis-server libsndfile1
```

**Windows:**
```powershell
# Install via winget
winget install FFmpeg
# Or use WSL2 (recommended)
```

### Step 2 â€” Install Ollama

```bash
# macOS / Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/download
```

### Step 3 â€” Pull required models

```bash
ollama pull llama3.2
ollama pull nomic-embed-text
```

Verify models are available:
```bash
ollama list
```

### Step 4 â€” Start Redis

```bash
# macOS
brew services start redis

# Linux
sudo systemctl start redis
# or
redis-server --daemonize yes

# Verify
redis-cli ping  # should return PONG
```

### Step 5 â€” Set up Python environment

```bash
# Python 3.11 recommended
python3 -m venv .venv
source .venv/bin/activate       # Linux/macOS
# .venv\Scripts\activate       # Windows

pip install -r requirements.txt
```

### Step 6 â€” Configure environment

```bash
cp .env.example .env
```

Edit `.env` for local dev:
```env
OLLAMA_BASE_URL=http://localhost:11434
REDIS_URL=redis://localhost:6379
DEBUG=true
```

### Step 7 â€” Run the server

```bash
# From the project root
python -m uvicorn server.main:app --host 0.0.0.0 --port 8000 --reload
```

### Step 8 â€” Open the UI

Navigate to **http://localhost:8000**

---

## ğŸ“š Adding Documents to the Knowledge Base

The RAG system lets your agent answer questions from your own documents.

### Via the Web UI

1. Open the sidebar (right panel)
2. Paste text into "Ingest Document"
3. Click "Add to Knowledge Base"

### Via the API

```bash
curl -X POST http://localhost:8000/api/ingest \
  -H "Content-Type: application/json" \
  -d '{"text": "Your document content here...", "source": "my-doc"}'
```

### Via the file system

Drop files into `data/documents/` â€” supported formats:
- `.txt` â€” plain text
- `.md` â€” Markdown
- `.pdf` â€” PDF (text-extractable)
- `.json` â€” JSON with `content` or `text` field

Documents are **automatically ingested** when the server starts if the FAISS index is empty.

**Force re-index** existing documents:
```bash
# Delete the index and restart
rm -rf data/faiss_index/*
docker compose restart voice-agent  # or restart the server
```

---

## ğŸ”§ Configuration

All settings are in `.env`. Key options:

### Change the LLM model

```env
OLLAMA_MODEL=mistral        # Or phi3.5, qwen2.5:7b, etc.
```

Then pull the model: `ollama pull mistral`

### Change the TTS voice

```env
TTS_VOICE=en-GB-SoniaNeural   # British female
TTS_VOICE=en-US-GuyNeural     # US male
```

List available voices via API: `GET /api/voices`

### Use a larger Whisper model (better accuracy)

```env
WHISPER_MODEL=small.en    # More accurate but slower
WHISPER_MODEL=medium.en   # Best accuracy for English
```

### GPU acceleration

For CUDA GPU support:
```env
WHISPER_DEVICE=cuda
WHISPER_COMPUTE_TYPE=float16
```

Uncomment the GPU section in `docker-compose.yml` for Ollama GPU:
```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]
```

---

## ğŸ”’ HTTPS Setup

Chrome blocks microphone on non-localhost HTTP. For remote deployment:

### Option A â€” Self-signed cert (development)

```bash
mkdir infra/certs
openssl req -x509 -newkey rsa:4096 -keyout infra/certs/key.pem \
  -out infra/certs/cert.pem -days 365 -nodes \
  -subj "/CN=voice-agent"
```

Update `nginx.conf`:
```nginx
listen 443 ssl;
ssl_certificate /etc/nginx/certs/cert.pem;
ssl_certificate_key /etc/nginx/certs/key.pem;
```

### Option B â€” Let's Encrypt (production)

```bash
certbot certonly --webroot -w /var/www/html -d yourdomain.com
```

---

## ğŸ“¡ API Reference

| Endpoint | Method | Description |
|----------|--------|-------------|
| `GET /api/health` | GET | Full health check with service status |
| `GET /api/health/ready` | GET | Simple readiness probe |
| `POST /api/ingest` | POST | Add text to knowledge base |
| `GET /api/voices` | GET | List available TTS voices |
| `WS /ws/audio` | WS | Main audio streaming endpoint |
| `WS /ws/webrtc/{id}` | WS | WebRTC signaling (optional) |

### WebSocket Protocol

**Client â†’ Server:**

```jsonc
// Start session
{ "type": "session_start", "data": { "format": "pcm16", "sample_rate": 16000 } }

// Binary audio frames (raw PCM Int16 LE)

// Signal end of speech
{ "type": "audio_end", "timestamp": 1234567890 }

// Keep-alive
{ "type": "ping", "timestamp": 1234567890 }
```

**Server â†’ Client:**

```jsonc
// Session established
{ "type": "session_start", "data": { "session_id": "uuid" } }

// Final transcript
{ "type": "transcript", "data": { "text": "Hello world", "is_final": true } }

// Streaming LLM text tokens
{ "type": "response_text", "data": { "text": "token", "is_final": false } }

// Audio chunk (base64 MP3)
{ "type": "response_audio", "data": { "audio": "base64...", "sample_rate": 24000 } }

// Audio stream complete
{ "type": "response_audio_end" }
```

---

## ğŸ› Troubleshooting

### "Could not transcribe audio"
- Check microphone permissions in browser
- Ensure HTTPS or localhost
- Try a louder/clearer voice input

### "Ollama connection failed"
```bash
# Check Ollama is running
curl http://localhost:11434/api/tags

# Pull required model
ollama pull llama3.2
```

### "Redis unavailable" (warning, non-fatal)
The app works without Redis â€” just without caching and persistent memory.
```bash
redis-cli ping  # Should return PONG
```

### Docker: models stuck downloading
```bash
docker compose logs -f ollama
docker compose logs -f ollama-init
```

### Slow responses
- Switch to a smaller Whisper model: `WHISPER_MODEL=tiny.en`
- Switch to a smaller LLM: `OLLAMA_MODEL=phi3.5`
- Enable GPU if available

### View logs
```bash
# All services
docker compose logs -f

# Just the API
docker compose logs -f voice-agent
```

---

## ğŸ“ Project Structure

```
voice-agent/
â”œâ”€â”€ client/              # TypeScript client library
â”‚   â”œâ”€â”€ web/
â”‚   â”‚   â”œâ”€â”€ microphone.ts    # Mic capture + silence detection
â”‚   â”‚   â”œâ”€â”€ audio-player.ts  # PCM/MP3 playback queue
â”‚   â”‚   â”œâ”€â”€ webrtc.ts        # WebRTC peer connection
â”‚   â”‚   â””â”€â”€ websocket.ts     # Main WS client + state machine
â”‚   â””â”€â”€ config.ts
â”‚
â”œâ”€â”€ server/              # Python FastAPI backend
â”‚   â”œâ”€â”€ main.py              # App entrypoint + lifecycle
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ websocket_handler.py  # WS audio pipeline
â”‚   â”‚   â”œâ”€â”€ webrtc_handler.py     # WebRTC signaling
â”‚   â”‚   â””â”€â”€ health_check.py       # Health endpoints
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ voice_agent.py    # Orchestrates STTâ†’RAGâ†’LLMâ†’TTS
â”‚   â”‚   â”œâ”€â”€ rag_agent.py      # Document ingestion + retrieval
â”‚   â”‚   â””â”€â”€ memory_agent.py   # Conversation memory
â”‚   â”œâ”€â”€ speech/
â”‚   â”‚   â”œâ”€â”€ stt.py            # Whisper speech-to-text
â”‚   â”‚   â”œâ”€â”€ tts.py            # Edge-TTS text-to-speech
â”‚   â”‚   â””â”€â”€ audio_processor.py
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ ollama_client.py  # Ollama HTTP client
â”‚   â”‚   â””â”€â”€ prompt_builder.py
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”œâ”€â”€ embedding_client.py    # Ollama + fallback embeddings
â”‚   â”‚   â””â”€â”€ embedding_pipeline.py  # Document chunking + embedding
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ vector_store.py   # FAISS index
â”‚   â”‚   â”œâ”€â”€ retriever.py      # Semantic search
â”‚   â”‚   â””â”€â”€ document_loader.py
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ conversation_store.py  # Redis-backed session store
â”‚   â”‚   â””â”€â”€ memory_retriever.py    # Auto-summarization
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â””â”€â”€ redis_cache.py
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ settings.py    # Pydantic settings
â”‚       â””â”€â”€ constants.py   # Prompts, enums
â”‚
â”œâ”€â”€ static/              # Web UI (served by FastAPI)
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/       # Drop files here for RAG
â”‚   â””â”€â”€ faiss_index/     # Auto-generated FAISS index
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ nginx/nginx.conf
â”‚   â””â”€â”€ ollama/ollama-init.sh
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model_config.yaml   # Available model reference
â”‚   â””â”€â”€ ollama_models.md
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## License

MIT
